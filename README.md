# Qwen2.5VL-R1

ğŸš€ **Qwen2.5VL-R1** is an optimized variant of [Qwen2.5-VL](https://github.com/QwenLM/Qwen-VL) tailored for **video classification with reasoning** using Chain-of-Thought (CoT) prompting and visual grounding.

---

## ğŸ§  Key Features

- ğŸ“¹ **Video Reasoning Dataset Generator**  
  Create synthetic motion-based video clips labeled with direction + reasoning steps using GPT-4V.

- ğŸ§ª **LoRA Fine-tuning Support**  
  Lightweight fine-tuning of Qwen2.5-VL with visual adapters using [DeepSpeed ZeRO-2](https://www.deepspeed.ai/tutorials/zero/).

- ğŸ§  **Multimodal Reasoning Engine**  
  Custom forward methods for mixed visual/textual tokens and temporal CoT-style reasoning.

---

## ğŸ—‚ï¸ Directory Structure

```
qwen2.5VL-R1/
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ video_generator.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_training.py
â”‚   â”œâ”€â”€ demo.py
â”‚   â””â”€â”€ zero2_offload.json
â””â”€â”€ src/
    â””â”€â”€ training/
        â”œâ”€â”€ train.py
        â”œâ”€â”€ train_grpo.py
        â”œâ”€â”€ data.py
        â”œâ”€â”€ trainer.py
        â”œâ”€â”€ params.py
        â”œâ”€â”€ modality_patch.py
        â””â”€â”€ constants.py
```

---

## ğŸ”§ Setup

```bash
# Clone repo
git clone https://github.com/yourname/qwen2.5VL-R1.git
cd qwen2.5VL-R1

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“¼ Generate Dataset

```bash
python video_generator.py \
  --output_dir ./data/synthetic_videos \
  --num_samples 20 \
  --cot \
  --frame_size 64 \
  --video_length 30 \
  --split 0.8
```

> âš ï¸ Requires `OPENAI_API_KEY` in your environment if `--cot` is enabled.

---

## ğŸ§ª Fine-tuning (LoRA)

Run the unified training CLI with the following options:

### â–¶ï¸ Regular LoRA fine-tuning

```bash
python scripts/run_finetuning.py \
  --model_id Qwen/Qwen2.5-VL-3B-Instruct \
  --data_path ./data/synthetic_videos/train.json \
  --image_folder ./data/synthetic_videos/videos \
  --output_dir ./output/video_lora
```

### â–¶ï¸ GRPO fine-tuning

```bash
python scripts/run_training.py \
  --use_grpo True \
  --model_id Qwen/Qwen2.5-VL-3B-Instruct \
  --data_path ./data/synthetic_videos/train.json \
  --image_folder ./data/synthetic_videos/videos \
  --output_dir ./output/grpo_video_lora
```

You can tweak additional args like `--batch_size`, `--lr`, `--lora_rank`, `--fps`, and `--video_max_pixels`.

---

## ğŸ§  Inference Demo

```bash
python scripts/demo.py \
  --video_path ./data/synthetic_videos/videos/000.mp4 \
  --prompt "What is happening in this video?"
```

---

## ğŸ“ Notes

- Base model: `Qwen/Qwen2.5-VL-3B-Instruct`
- Chain-of-Thought reasoning uses `<think>` and `<answer>` tags
- Supports DeepSpeed + FlashAttention for scalable training

---

## ğŸ“š Credits

- [Qwen2.5-VL](https://github.com/QwenLM/Qwen-VL) by Alibaba DAMO
- PEFT + LoRA via [Hugging Face PEFT](https://github.com/huggingface/peft)

---

## ğŸ›¡ License

MIT (or inherit from base model â€” update as appropriate)
